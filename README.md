# ğŸš€ Real-Time Anomaly Detection â€“ MLOps System

This project demonstrates a production-ready, cloud-native MLOps pipeline that automates the full lifecycle of a real-time anomaly detection model using modern DevOps, GitOps, and ML tooling. It is designed to:

- Showcase end-to-end MLOps expertise to recruiters, engineers, and hiring managers  
- Automate everything from training to deployment using CI/CD and GitHub Actions  
- Enable retraining and redeployment based on drift detection  
- Integrate observability and multi-layer security scanning  
- Use only scalable, industry-standard, cloud-native tools

---

## ğŸ§° Tech Stack

| Category         | Tool                               | Usage                                  |
|------------------|------------------------------------|----------------------------------------|
| Cloud            | AWS (EKS, S3, IAM, VPC)            | Hosting, storage, roles                |
| IaC              | Terraform                          | VPC, EKS, IAM, S3 (with remote backend)|
| CI/CD            | GitHub Actions                     | Model training, image build, chart publishing |
| GitOps           | ArgoCD                             | Helm-based continuous delivery         |
| ML/Serving       | IsolationForest + FastAPI          | Anomaly detection + REST API           |
| Tracking         | MLflow + S3 + SQLite               | Logs metrics/artifacts, tracks versions|
| Containerization | Docker + Docker Hub                | Build & registry                       |
| Monitoring       | Prometheus + Grafana               | Metrics, dashboards, alerts            |
| Drift Detection  | Custom logic + CronJob             | Detects drift and triggers retraining  |
| Security         | tfsec, TFLint, Trivy, CodeQL       | Infra, container, and code scanning    |

---

## ğŸ“ System Architecture

![System Architecture](screenshots/system-design-flow-chart.png)

---

## ğŸ”„ End-to-End Pipeline

1. `train-model.yml` runs on schedule or manual dispatch
2. Trains IsolationForest model
3. Logs to MLflow and uploads artifacts to S3
4. Creates PR to update `model.pkl` in inference service
5. Merge triggers `build-deploy.yml`
6. Builds and pushes Docker image to Docker Hub
7. Creates PR to update image tag in `helm/values.yaml`
8. Merge triggers `upload-helm.yml`
9. Uploads Helm chart and index to S3 bucket
10. ArgoCD watches S3 chart index and syncs deployment to EKS

---

## âš™ï¸ CI/CD Workflows

| Workflow           | Trigger                         | Purpose                                      |
|--------------------|----------------------------------|----------------------------------------------|
| terraform.yml      | Push to terraform/** or manual  | Plan/apply infrastructure changes            |
| train-model.yml    | Manual dispatch or Cron         | Train model, log to MLflow, PR update model.pkl |
| build-deploy.yml   | Push to inference-api or model  | Build/push Docker image, PR update Helm tag  |
| upload-helm.yml    | Helm chart update               | Upload Helm chart and index to S3            |
| security.yml       | Every push to main              | tfsec, TFLint, Trivy, CodeQL scans           |

---

## ğŸ“¦ Breakdown of Workflow Responsibilities

- âœ… **train-model.yml**  
  - Trains model with `train.py`  
  - Logs to MLflow and uploads to S3  
  - Creates PR to update `model.pkl`  

- âœ… **build-deploy.yml**  
  - Builds and pushes Docker image to Docker Hub  
  - Opens PR to update Helm image tag  

- âœ… **upload-helm.yml**  
  - Packages and uploads Helm chart to S3  

- âœ… **terraform.yml**  
  - Formats, initializes, validates, and plans infra  
  - Applies infra manually via GitHub Actions approval  

- âœ… **security.yml**  
  - Runs tfsec, TFLint, Trivy, and CodeQL  

---

## ğŸ› ï¸ Infrastructure with Terraform

- Modularized under `terraform/modules/`
- `terraform/envs/dev/` handles:
  - Remote state in S3
  - State locking via DynamoDB
- Provisions:
  - VPC, subnets, NAT, IGW
  - IAM roles
  - EKS cluster + node group
  - S3 buckets for:
    - `model.pkl`
    - Helm chart repository

---

## ğŸš€ Deployment via ArgoCD + Helm

- Helm charts in `helm/inference-api/`
- Image tag updates via PR â†’ merged
- ArgoCD tracks `argocd/inference-app.yaml`
- ArgoCD syncs deployment after chart update

![ArgoCD](screenshots/ARGOCD.png)

---

## ğŸ³ Docker

- Two Dockerfiles:
  - `services/inference-api/`
  - `model/scripts/train.py`
- Images pushed to: `gantagouthamyadav/inference-api`

![Docker Hub](screenshots/docker-hub.png)

---

## ğŸ“Š Monitoring with Prometheus + Grafana

- Prometheus scrapes `/metrics` from FastAPI
- Grafana visualizes alerts, container metrics, and usage

![Prometheus](screenshots/prometheus.png)

---

## ğŸ“˜ MLflow Tracking

- Logs parameters, metrics, and artifacts
- Stores models in S3
- Uses SQLite for lightweight backend

![MLflow](screenshots/Mlflow.png)

---

## ğŸ§ª FastAPI Inference + Drift Logic

- REST endpoint accepts input features
- Returns prediction + drift detection score

![FastAPI](screenshots/git-repo.png)

---

## ğŸ“‚ Folder Structure

```bash
mlops-anomaly-detection/
â”œâ”€â”€ .github/workflows/       # GitHub Actions: CI/CD, Security
â”œâ”€â”€ argocd/                  # ArgoCD app manifest
â”œâ”€â”€ helm/                    # Helm chart (FastAPI)
â”œâ”€â”€ model/                   # Training pipeline
â”‚   â””â”€â”€ scripts/train.py
â”œâ”€â”€ services/                # Inference API (FastAPI)
â”œâ”€â”€ terraform/               # IaC (modular)
â”‚   â”œâ”€â”€ modules/
â”‚   â””â”€â”€ envs/dev/
â”œâ”€â”€ simulate_stream.py       # Mock streaming simulator
â”œâ”€â”€ screenshots/             # Proof of working implementation
âœ… Results
âœ… Fully automated MLOps lifecycle

âœ… Drift triggers retraining via Cron + GitHub API

âœ… Monitoring and alerting integrated

âœ… Secure infrastructure and container scanning

âœ… GitOps delivery with Helm + ArgoCD

âœ… Clean, reproducible, scalable architecture

ğŸ§¯ Troubleshooting
Area	Problem	Fix
MLflow	Upload fails to S3	Check AWS credentials via Kubernetes Secret
ArgoCD	Not syncing	Ensure Helm PR is merged and chart pushed
Grafana	Login issue	Reset Bitnami credentials
CronJob	Not triggering	Run kubectl get cronjob -n mlops
GitHub PR	Not created	Ensure GH_PAT is set in GitHub secrets

ğŸ§ª Local Testing
bash
Always show details

Copy
# Clone the project
git clone https://github.com/gouthamyadavganta/mlops-anomaly-detection.git
cd mlops-anomaly-detection

# Train model locally
python model/scripts/train.py

# Start FastAPI app
cd services/inference-api
uvicorn main:app --reload

# Simulate streaming
python simulate_stream.py
ğŸ§  What Can Be Improved
Add pytest unit tests for FastAPI and model

Use IRSA instead of Kubernetes secrets for AWS

Replace SQLite with PostgreSQL (RDS) for MLflow

Add Kafka or Kinesis for real streaming

Add load testing with Locust or k6

ğŸ“š References
Terraform

MLflow

FastAPI

ArgoCD

Prometheus

Grafana

Trivy

CodeQL
"""
